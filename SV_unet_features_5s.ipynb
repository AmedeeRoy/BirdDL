{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from utils.trip import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('./data/SV_train.csv')\n",
    "data_validation = pd.read_csv('./data/SV_validation.csv')\n",
    "data_test = pd.read_csv('./data/SV_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip</th>\n",
       "      <th>datetime</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>gaps</th>\n",
       "      <th>dive</th>\n",
       "      <th>step_speed</th>\n",
       "      <th>step_direction</th>\n",
       "      <th>lon_std</th>\n",
       "      <th>lat_std</th>\n",
       "      <th>step_speed_std</th>\n",
       "      <th>step_direction_cos</th>\n",
       "      <th>step_direction_sin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P1108_46_SV_T4</td>\n",
       "      <td>2008-12-06 16:06:25</td>\n",
       "      <td>-77.265897</td>\n",
       "      <td>-11.774297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.301035</td>\n",
       "      <td>26.025648</td>\n",
       "      <td>2.309584</td>\n",
       "      <td>0.788287</td>\n",
       "      <td>0.826924</td>\n",
       "      <td>0.898598</td>\n",
       "      <td>0.438773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P1108_46_SV_T4</td>\n",
       "      <td>2008-12-06 16:06:30</td>\n",
       "      <td>-77.266435</td>\n",
       "      <td>-11.774462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.287117</td>\n",
       "      <td>-18.228550</td>\n",
       "      <td>2.293957</td>\n",
       "      <td>0.776022</td>\n",
       "      <td>0.663327</td>\n",
       "      <td>0.949816</td>\n",
       "      <td>-0.312808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P1108_46_SV_T4</td>\n",
       "      <td>2008-12-06 16:06:35</td>\n",
       "      <td>-77.266843</td>\n",
       "      <td>-11.774868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.679214</td>\n",
       "      <td>-28.073536</td>\n",
       "      <td>2.282105</td>\n",
       "      <td>0.745841</td>\n",
       "      <td>0.684610</td>\n",
       "      <td>0.882344</td>\n",
       "      <td>-0.470604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P1108_46_SV_T4</td>\n",
       "      <td>2008-12-06 16:06:40</td>\n",
       "      <td>-77.267200</td>\n",
       "      <td>-11.775312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.579325</td>\n",
       "      <td>-6.324066</td>\n",
       "      <td>2.271735</td>\n",
       "      <td>0.712835</td>\n",
       "      <td>0.679188</td>\n",
       "      <td>0.993915</td>\n",
       "      <td>-0.110152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P1108_46_SV_T4</td>\n",
       "      <td>2008-12-06 16:06:45</td>\n",
       "      <td>-77.267390</td>\n",
       "      <td>-11.775845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.567659</td>\n",
       "      <td>-18.969971</td>\n",
       "      <td>2.266216</td>\n",
       "      <td>0.673214</td>\n",
       "      <td>0.678555</td>\n",
       "      <td>0.945689</td>\n",
       "      <td>-0.325073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             trip             datetime        lon        lat  gaps  dive  \\\n",
       "2  P1108_46_SV_T4  2008-12-06 16:06:25 -77.265897 -11.774297   0.0     0   \n",
       "3  P1108_46_SV_T4  2008-12-06 16:06:30 -77.266435 -11.774462   0.0     0   \n",
       "4  P1108_46_SV_T4  2008-12-06 16:06:35 -77.266843 -11.774868   0.0     0   \n",
       "5  P1108_46_SV_T4  2008-12-06 16:06:40 -77.267200 -11.775312   0.0     0   \n",
       "6  P1108_46_SV_T4  2008-12-06 16:06:45 -77.267390 -11.775845   0.0     0   \n",
       "\n",
       "   step_speed  step_direction   lon_std   lat_std  step_speed_std  \\\n",
       "2   15.301035       26.025648  2.309584  0.788287        0.826924   \n",
       "3   12.287117      -18.228550  2.293957  0.776022        0.663327   \n",
       "4   12.679214      -28.073536  2.282105  0.745841        0.684610   \n",
       "5   12.579325       -6.324066  2.271735  0.712835        0.679188   \n",
       "6   12.567659      -18.969971  2.266216  0.673214        0.678555   \n",
       "\n",
       "   step_direction_cos  step_direction_sin  \n",
       "2            0.898598            0.438773  \n",
       "3            0.949816           -0.312808  \n",
       "4            0.882344           -0.470604  \n",
       "5            0.993915           -0.110152  \n",
       "6            0.945689           -0.325073  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolution = 5\n",
    "\n",
    "data_train_new = change_resolution(data_train, resolution)\n",
    "data_validation_new = change_resolution(data_validation, resolution)\n",
    "data_test_new = change_resolution(data_test, resolution)\n",
    "\n",
    "data_train_new = standardize_data(data_train_new)\n",
    "data_validation_new = standardize_data(data_validation_new)\n",
    "data_test_new = standardize_data(data_test_new)\n",
    "\n",
    "data_train_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "window = 20\n",
    "variable = ('lon_std', 'lat_std', 'gaps')\n",
    "\n",
    "train_set = TrajDataSet(data_train_new, window, variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64\n",
    "\n",
    "## reduce size dataset\n",
    "train_set = TrajDataSet(data_train_new, window, variable, transform = ToTensor())\n",
    "validation_set = TrajDataSet(data_validation_new, window, variable, transform = ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers = 0, shuffle = True, drop_last=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, num_workers = 0, shuffle = True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, nb_features):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.feature = nb_features\n",
    "        \n",
    "        self.threshold = nn.Sequential(\n",
    "#             nn.BatchNorm2d(1),\n",
    "            nn.Conv2d(1, self.feature, kernel_size = 1, stride = 1, padding = 0, dilation = 1, bias = True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.cnn_input_1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.feature+3),\n",
    "            nn.Conv1d(self.feature+3, 8, kernel_size = 5, stride = 1, padding = 2, dilation = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 8, kernel_size = 5, stride = 1, padding = 2, dilation = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.pooling_1 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size = 5, stride = 2, padding = 2, dilation = 1)\n",
    "        )\n",
    "\n",
    "        self.cnn_input_2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.Conv1d(8, 16, kernel_size = 5, stride = 1, padding = 2, dilation = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, kernel_size = 5, stride = 1, padding = 2, dilation = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.pooling_2 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size = 5, stride = 2, padding = 2, dilation = 1)\n",
    "        )\n",
    "\n",
    "        self.cnn_input_3 = nn.Sequential(\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Conv1d(16, 32,  kernel_size = 5, stride = 1, padding = 2, dilation = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32,  kernel_size = 5, stride = 1, padding = 2, dilation = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.upconv_2 = nn.Sequential(\n",
    "             nn.ConvTranspose1d(32, 16, kernel_size = 6, stride = 2, padding = 2, dilation = 1)\n",
    "         )\n",
    "\n",
    "        self.cnn_output_2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(16*2),\n",
    "            nn.Conv1d(16*2, 16,  kernel_size = 5, stride = 1, padding = 2, dilation = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16,  kernel_size = 5, stride = 1, padding = 2, dilation = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.upconv_1 = nn.Sequential(\n",
    "             nn.ConvTranspose1d(16, 8, kernel_size = 6, stride = 2, padding = 2, dilation = 1)\n",
    "         )\n",
    "        \n",
    "        self.cnn_output_1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(8*2),\n",
    "            nn.Conv1d(8*2, 8,  kernel_size = 5, stride = 1, padding = 2, dilation = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 4, kernel_size = 5, stride = 1, padding = 2, dilation = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(4, 2, kernel_size = 5, stride = 1, padding = 2, dilation = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(2, 1,  kernel_size = 5, stride = 1, padding = 2, dilation = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = self.threshold(y)\n",
    "        out = torch.sum(out, 2)\n",
    "\n",
    "        out = torch.cat((out, x.squeeze(1)), 1)\n",
    "\n",
    "        out_1 = self.cnn_input_1(out)\n",
    "        out = self.pooling_1(out_1)\n",
    "        out_2 = self.cnn_input_2(out)\n",
    "        out = self.pooling_2(out_2)\n",
    "        out = self.cnn_input_3(out)\n",
    "\n",
    "        out = self.upconv_2(out)\n",
    "        out = torch.cat((out, out_2), 1)\n",
    "        out = self.cnn_output_2(out)\n",
    "\n",
    "        out = self.upconv_1(out)\n",
    "        out = torch.cat((out, out_1), 1)\n",
    "        out = self.cnn_output_1(out)\n",
    "\n",
    "\n",
    "        return out\n",
    "\n",
    "def get_score(out, y):\n",
    "    out, y = out.cpu(), y.cpu()\n",
    "    out = 1*(out>0)\n",
    "    true_positive = np.mean(out[y == True].numpy()) \n",
    "    true_negative = 1-np.mean(out[y == False].numpy())\n",
    "    \n",
    "    return (round(true_positive*100) , round(true_negative*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8d4aba7c83cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Backprop and perform optimisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters = []\n",
    "for feature in [1, 2, 4, 8]:\n",
    "\n",
    "    weight = torch.FloatTensor([30])\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    # switch to GPU\n",
    "    model = UNet(feature)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    weight = weight.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight = weight)\n",
    "    \n",
    "    epoch_loss_train = []\n",
    "    epoch_loss_validation = []\n",
    "\n",
    "    for batch, (x, y, z) in enumerate(train_loader):\n",
    "        \n",
    "        model.train()\n",
    "        # send to GPU\n",
    "        x, y, z = x.to(device), y.to(device), z.to(device)\n",
    "        # Run the forward pass\n",
    "        out = model(x, y)\n",
    "        loss = criterion(out, z)\n",
    "\n",
    "        # Backprop and perform optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "\n",
    "    ### Evaluation + Validation every epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():      \n",
    "        j = 0\n",
    "        # evaluation\n",
    "        list_loss_train = []\n",
    "        list_score_train = []\n",
    "        for batch, (x, y, z) in enumerate(train_loader):\n",
    "            j+= 1\n",
    "            # send to GPU\n",
    "            x, y, z = x.to(device), y.to(device), z.to(device)\n",
    "\n",
    "            # Run the forward pass\n",
    "            out =  model(x, y)\n",
    "            loss = criterion(out, z)\n",
    "            score = get_score(out,z)\n",
    "            list_loss_train.append(loss.item())\n",
    "            list_score_train.append(score)\n",
    "\n",
    "        train_loss = np.mean(list_loss_train)\n",
    "        train_trueP = np.mean([tp for (tp, tn) in list_score_train])\n",
    "        train_trueN = np.mean([tn for (tp, tn) in list_score_train])\n",
    "\n",
    "        k = 0\n",
    "        # validation\n",
    "        list_loss_validation = []\n",
    "        list_score_validation = []\n",
    "        for batch, (x, y, z) in enumerate(validation_loader):\n",
    "            k+= 1\n",
    "            # send to GPU\n",
    "            x, y, z = x.to(device), y.to(device), z.to(device)\n",
    "\n",
    "            # Run the forward pass\n",
    "            out =  model(x, y)\n",
    "            loss = criterion(out, z)\n",
    "            score = get_score(out,z)\n",
    "            list_loss_validation.append(loss.item())\n",
    "            list_score_validation.append(score)\n",
    "\n",
    "        validation_loss = np.mean(list_loss_validation)\n",
    "        validation_trueP = np.mean([tp for (tp, tn) in list_score_validation])\n",
    "        validation_trueN = np.mean([tn for (tp, tn) in list_score_validation])\n",
    "\n",
    "    epoch_loss_train.append(train_loss)\n",
    "    epoch_loss_validation.append(validation_loss)\n",
    "    \n",
    "    param = - model.threshold[0].bias.squeeze().detach().numpy() / model.threshold[0].weight.squeeze().detach().numpy()\n",
    "    parameters.append(param)\n",
    "    \n",
    "    print('Feature [{}] -------------------------------------------------------------------------------------'\n",
    "          .format(feature))\n",
    "    print('Train Loss: {}, Train True Positive : {} %, Train True Negative : {} %'\n",
    "            .format(round(train_loss, 2), round(train_trueP, 2), round(train_trueN, 2)))\n",
    "    print('Validation Loss: {}, Validation True Positive : {} %, Validation True Negative : {} %'\n",
    "            .format(round(validation_loss, 2), round(validation_trueP, 2), round(validation_trueN, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
