{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "import datetime\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import trip\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to database\n",
    "conn = sqlite3.connect('/home/amdroy/MEGA/DATA/seabirdbank.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip</th>\n",
       "      <th>datetime</th>\n",
       "      <th>pressure</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P1108_4_SV_T1</td>\n",
       "      <td>2008-11-25 13:00:35</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-77.264128</td>\n",
       "      <td>-11.773317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P1108_4_SV_T1</td>\n",
       "      <td>2008-11-25 13:00:36</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-77.264118</td>\n",
       "      <td>-11.773235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P1108_4_SV_T1</td>\n",
       "      <td>2008-11-25 13:00:37</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-77.264098</td>\n",
       "      <td>-11.773152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P1108_4_SV_T1</td>\n",
       "      <td>2008-11-25 13:00:38</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-77.264113</td>\n",
       "      <td>-11.773060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P1108_4_SV_T1</td>\n",
       "      <td>2008-11-25 13:00:39</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-77.264142</td>\n",
       "      <td>-11.772955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862916</th>\n",
       "      <td>P1113_50_SV_T1</td>\n",
       "      <td>2013-11-24 13:09:17</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-77.264229</td>\n",
       "      <td>-11.773601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862917</th>\n",
       "      <td>P1113_50_SV_T1</td>\n",
       "      <td>2013-11-24 13:09:18</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>-77.264221</td>\n",
       "      <td>-11.773701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862918</th>\n",
       "      <td>P1113_50_SV_T1</td>\n",
       "      <td>2013-11-24 13:09:19</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>-77.264191</td>\n",
       "      <td>-11.773771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862919</th>\n",
       "      <td>P1113_50_SV_T1</td>\n",
       "      <td>2013-11-24 13:09:20</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-77.264168</td>\n",
       "      <td>-11.773826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862920</th>\n",
       "      <td>P1113_50_SV_T1</td>\n",
       "      <td>2013-11-24 13:09:21</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-77.264160</td>\n",
       "      <td>-11.773853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>862921 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  trip            datetime  pressure        lon        lat\n",
       "0        P1108_4_SV_T1 2008-11-25 13:00:35     -0.35 -77.264128 -11.773317\n",
       "1        P1108_4_SV_T1 2008-11-25 13:00:36     -0.38 -77.264118 -11.773235\n",
       "2        P1108_4_SV_T1 2008-11-25 13:00:37     -0.35 -77.264098 -11.773152\n",
       "3        P1108_4_SV_T1 2008-11-25 13:00:38     -0.29 -77.264113 -11.773060\n",
       "4        P1108_4_SV_T1 2008-11-25 13:00:39     -0.29 -77.264142 -11.772955\n",
       "...                ...                 ...       ...        ...        ...\n",
       "862916  P1113_50_SV_T1 2013-11-24 13:09:17     -1.01 -77.264229 -11.773601\n",
       "862917  P1113_50_SV_T1 2013-11-24 13:09:18     -1.07 -77.264221 -11.773701\n",
       "862918  P1113_50_SV_T1 2013-11-24 13:09:19     -1.07 -77.264191 -11.773771\n",
       "862919  P1113_50_SV_T1 2013-11-24 13:09:20     -1.01 -77.264168 -11.773826\n",
       "862920  P1113_50_SV_T1 2013-11-24 13:09:21     -1.01 -77.264160 -11.773853\n",
       "\n",
       "[862921 rows x 5 columns]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request = \"SELECT \\\n",
    "            trip.id as trip, gps.datetime, gps.lon, gps.lat\\\n",
    "            FROM gps \\\n",
    "            INNER JOIN trip ON gps.trip = trip.id \\\n",
    "            INNER JOIN bird ON trip.bird = bird.id \\\n",
    "            WHERE trip.file_gps IS NOT NULL \\\n",
    "            AND trip.file_tdr IS NOT NULL \\\n",
    "            AND (bird.fieldwork = 'P1108' OR bird.fieldwork = 'P1111' \\\n",
    "                OR bird.fieldwork = 'P1112' OR bird.fieldwork = 'P1113') \\\n",
    "            AND bird.species = 'SV'\"\n",
    "\n",
    "# substr(bird.fieldwork,1,1) = 'P'\n",
    "\n",
    "gps = pd.read_sql_query(request, conn)\n",
    "\n",
    "# check duplicated datetime in GPS trip\n",
    "idx = gps.index[gps.loc[:, 'trip':'datetime'].duplicated(keep=False)]\n",
    "idx_drop = [idx[i] for i in range(0, len(idx), 2)]\n",
    "gps = gps.drop(idx_drop)\n",
    "\n",
    "request = \"SELECT \\\n",
    "            trip.id as trip, tdr.datetime, tdr.pressure \\\n",
    "            FROM tdr \\\n",
    "            INNER JOIN trip ON tdr.trip = trip.id \\\n",
    "            INNER JOIN bird ON trip.bird = bird.id \\\n",
    "            WHERE trip.file_gps IS NOT NULL \\\n",
    "            AND trip.file_tdr IS NOT NULL \\\n",
    "            AND (bird.fieldwork = 'P1108' OR bird.fieldwork = 'P1111' \\\n",
    "                OR bird.fieldwork = 'P1112' OR bird.fieldwork = 'P1113') \\\n",
    "            AND bird.species = 'SV'\"\n",
    "\n",
    "# WHERE substr(bird.fieldwork,1,1) = 'P' \n",
    "\n",
    "tdr = pd.read_sql_query(request, conn)\n",
    "tdr['datetime'] = pd.to_datetime(tdr['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "gps['datetime'] = pd.to_datetime(gps['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "df = pd.merge(tdr, gps, on=['trip', 'datetime'], how='left')\n",
    "\n",
    "\n",
    "# check NaN and fill with interpolations\n",
    "[i for i, x in enumerate(df['lon'].isna()) if x]\n",
    "\n",
    "df['lon'] = df['lon'].interpolate(method='nearest', limit_direction='backward')\n",
    "df['lat'] = df['lat'].interpolate(method='nearest', limit_direction='backward')\n",
    "\n",
    "bad_trip = ['P1111_9_SV_T1', 'P1111_13_SV_T6', 'P1111_41_SV_T2', 'P1111_46_SV_T5', 'P1111_52_SV_T1',\n",
    "            'P1112_4_SV_T2', 'P1112_26_SV_T5', 'P1112_36_SV_T1', 'P1112_36_SV_T2', 'P1113_41_SV_T3',\n",
    "           'P1113_50_SV_T2']\n",
    "\n",
    "no_loop = ['P1112_26_SV_T1', 'P1112_26_SV_T2', 'P1112_26_SV_T3', 'P1112_26_SV_T4']\n",
    "\n",
    "### remove bad data\n",
    "idx = df.index[[(t in bad_trip or t in no_loop) for t in df.trip]]\n",
    "df = df.drop(idx)\n",
    "\n",
    "### reindex data\n",
    "df = df.set_index(np.arange(len(df)))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epoch = 600\n",
    "batch_size = 8\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajDataSet(Dataset):\n",
    "    def __init__(self,  df, epoch, transform=None):\n",
    "        self.df = df\n",
    "        self.epoch = epoch\n",
    "        self.start_idx = np.where([self.df.trip[i]==self.df.trip[i+self.epoch] for i in range(len(self.df)-self.epoch)])[0]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.start_idx)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        i = self.start_idx[idx]\n",
    "        \n",
    "        traj = self.df.loc[i:i+self.epoch-1, ('lon', 'lat')]           \n",
    "        traj = np.array(traj).T\n",
    "        \n",
    "        \n",
    "        dive = self.df.loc[i:i+self.epoch-1, 'pressure']\n",
    "        dive = np.array(dive)\n",
    "        \n",
    "        sample = (traj, dive)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample\n",
    "\n",
    "class Threshold(object):\n",
    "    \"\"\"Define dives with threshold\n",
    "\n",
    "    Args:\n",
    "        threshold value \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        traj, dive = sample\n",
    "\n",
    "        # change resolution\n",
    "        dive_new = np.where( (dive - np.median(dive)) > self.threshold, 1, 0)\n",
    "\n",
    "        return (traj, dive_new)\n",
    "    \n",
    "    \n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the output in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ratio, method='max'):\n",
    "        self.ratio = ratio\n",
    "        self.method = method\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        traj, dive = sample\n",
    "\n",
    "        # change resolution\n",
    "        if self.method == 'max':\n",
    "            dive_new = [np.max(dive[i:i+self.ratio+1]) for i in range(len(dive)) if i%self.ratio==0]\n",
    "            \n",
    "        if self.method == 'mean':\n",
    "            dive_new = [np.mean(dive[i:i+self.ratio+1]) for i in range(len(dive)) if i%self.ratio==0]\n",
    "\n",
    "        return (traj, dive_new)\n",
    "    \n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        traj, dive = sample\n",
    "        return (torch.from_numpy(traj), torch.from_numpy(dive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 600])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = TrajDataSet(df, epoch, transform = \n",
    "                            transforms.Compose([Rescale(10, 'mean'), Threshold(1), ToTensor()]))\n",
    "(x,y) = dataset_train[0]\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-77.2641, -77.2641, -77.2641,  ..., -77.3376, -77.3377, -77.3379],\n",
       "        [-11.7733, -11.7732, -11.7732,  ..., -11.7703, -11.7703, -11.7704]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset_train, batch_size=4, num_workers = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn_input_1 = nn.Sequential(\n",
    "            nn.Conv1d(2, 8, kernel_size = 11, stride = 1, padding = 5, dilation = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 8, kernel_size = 11, stride = 1, padding = 5, dilation = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.pooling_1 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size = 11, stride = 2, padding = 5, dilation = 1)\n",
    "        )\n",
    "\n",
    "        self.cnn_input_2 = nn.Sequential(\n",
    "            nn.Conv1d(8, 16, kernel_size = 11, stride = 1, padding = 5, dilation = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, kernel_size = 11, stride = 1, padding = 5, dilation = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.pooling_2 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size = 11, stride = 2, padding = 5, dilation = 1)\n",
    "        )\n",
    "\n",
    "        self.cnn_input_3 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, kernel_size = 11, stride = 1, padding = 5, dilation = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, kernel_size = 11, stride = 1, padding = 5, dilation = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.pooling_3 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size = 11, stride = 2, padding = 5, dilation = 4)\n",
    "        )\n",
    "\n",
    "        self.cnn_4 = nn.Sequential(\n",
    "            nn.Conv1d(32, 16, kernel_size = 11, stride = 1, padding = 5, dilation = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 8, kernel_size = 11, stride = 1, padding = 5, dilation = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 1, kernel_size = 11, stride = 1, padding = 5, dilation = 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        out = self.cnn_input_1(x)\n",
    "        out = self.pooling_1(out)\n",
    "        out = self.cnn_input_2(out)\n",
    "        out = self.pooling_2(out)\n",
    "        out = self.cnn_input_3(out)\n",
    "        out = self.pooling_3(out)\n",
    "        out = self.cnn_4(out)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defined the train loop\n",
    "        criterion = nn.BCELoss()\n",
    "            \n",
    "        x, y = batch\n",
    "        out = model(x.float())\n",
    "        loss =  criterion(out, y.float())\n",
    "        \n",
    "        # Logging to TensorBoard by default\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name        | Type       | Params\n",
      "-------------------------------------------\n",
      "0 | cnn_input_1 | Sequential | 896   \n",
      "1 | pooling_1   | Sequential | 0     \n",
      "2 | cnn_input_2 | Sequential | 4 K   \n",
      "3 | pooling_2   | Sequential | 0     \n",
      "4 | cnn_input_3 | Sequential | 16 K  \n",
      "5 | pooling_3   | Sequential | 0     \n",
      "6 | cnn_4       | Sequential | 7 K   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6bb8eb9315c459ca4e2e5492f15524c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# init model\n",
    "model = ConvNet()\n",
    "\n",
    "# most basic trainer, uses good defaults (auto-tensorboard, checkpoints, logs, and more)\n",
    "# trainer = pl.Trainer(gpus=8) (if you have GPUs)\n",
    "trainer = pl.Trainer()\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
