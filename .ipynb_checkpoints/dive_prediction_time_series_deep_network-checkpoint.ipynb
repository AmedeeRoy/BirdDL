{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0C-QfH4LlCB"
   },
   "source": [
    "# Dive Prediction - Time Series Deep Network\n",
    "\n",
    "*Predicting Seabird Diving Behaviour from GPS data*\n",
    "\n",
    "This notebook trains a neural network to predict seabirds' dives.\n",
    "\n",
    "Networks' characteristics:\n",
    "\n",
    "* *Trajectory window* : 600s\n",
    "* *Output resolution*: 60s\n",
    "* *Representation of trajectories* : Time Series\n",
    "* *Layers* : deep convolutions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjVXwurrGghM"
   },
   "source": [
    "## connect to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cPOjT66sGf3Y",
    "outputId": "b80982f4-7bd3-4181-d99f-14661929ddbd"
   },
   "outputs": [],
   "source": [
    "# mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u2R6nZsEGp2v",
    "outputId": "a182cc59-1bd3-4d20-9a99-6c765a794f5b"
   },
   "outputs": [],
   "source": [
    "%cd drive/My\\ Drive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nPTCXkLdLlCC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from utils.trip import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YPCxjcNuLlCC"
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('./data/data_train.csv')\n",
    "data_validation = pd.read_csv('./data/data_validation.csv')\n",
    "data_test = pd.read_csv('./data/data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Zo9pKfxqLlCC"
   },
   "outputs": [],
   "source": [
    "data_train = standardize_data(data_train)\n",
    "data_validation = standardize_data(data_validation)\n",
    "data_test = standardize_data(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip</th>\n",
       "      <th>datetime</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>pressure</th>\n",
       "      <th>gaps</th>\n",
       "      <th>step_speed</th>\n",
       "      <th>step_direction</th>\n",
       "      <th>dive</th>\n",
       "      <th>lon_std</th>\n",
       "      <th>lat_std</th>\n",
       "      <th>step_speed_std</th>\n",
       "      <th>step_direction_cos</th>\n",
       "      <th>step_direction_sin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P1108_46_SV_T1</td>\n",
       "      <td>2008-12-05 15:12:41</td>\n",
       "      <td>-77.262442</td>\n",
       "      <td>-11.773072</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>False</td>\n",
       "      <td>7.224129</td>\n",
       "      <td>3.984122</td>\n",
       "      <td>0</td>\n",
       "      <td>1.452647</td>\n",
       "      <td>1.542331</td>\n",
       "      <td>0.246157</td>\n",
       "      <td>0.997583</td>\n",
       "      <td>0.069480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P1108_46_SV_T1</td>\n",
       "      <td>2008-12-05 15:12:42</td>\n",
       "      <td>-77.262447</td>\n",
       "      <td>-11.773147</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>False</td>\n",
       "      <td>8.366185</td>\n",
       "      <td>13.285157</td>\n",
       "      <td>0</td>\n",
       "      <td>1.452565</td>\n",
       "      <td>1.541362</td>\n",
       "      <td>0.285681</td>\n",
       "      <td>0.973238</td>\n",
       "      <td>0.229798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P1108_46_SV_T1</td>\n",
       "      <td>2008-12-05 15:12:43</td>\n",
       "      <td>-77.262482</td>\n",
       "      <td>-11.773217</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>False</td>\n",
       "      <td>8.675223</td>\n",
       "      <td>22.346828</td>\n",
       "      <td>0</td>\n",
       "      <td>1.451995</td>\n",
       "      <td>1.540458</td>\n",
       "      <td>0.296376</td>\n",
       "      <td>0.924899</td>\n",
       "      <td>0.380212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P1108_46_SV_T1</td>\n",
       "      <td>2008-12-05 15:12:44</td>\n",
       "      <td>-77.262517</td>\n",
       "      <td>-11.773293</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>False</td>\n",
       "      <td>9.279737</td>\n",
       "      <td>-1.813227</td>\n",
       "      <td>0</td>\n",
       "      <td>1.451424</td>\n",
       "      <td>1.539476</td>\n",
       "      <td>0.317297</td>\n",
       "      <td>0.999499</td>\n",
       "      <td>-0.031641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P1108_46_SV_T1</td>\n",
       "      <td>2008-12-05 15:12:45</td>\n",
       "      <td>-77.262518</td>\n",
       "      <td>-11.773372</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>False</td>\n",
       "      <td>8.794348</td>\n",
       "      <td>-23.557701</td>\n",
       "      <td>0</td>\n",
       "      <td>1.451408</td>\n",
       "      <td>1.538456</td>\n",
       "      <td>0.300499</td>\n",
       "      <td>0.916658</td>\n",
       "      <td>-0.399672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             trip             datetime        lon        lat  pressure   gaps  \\\n",
       "2  P1108_46_SV_T1  2008-12-05 15:12:41 -77.262442 -11.773072     -0.26  False   \n",
       "3  P1108_46_SV_T1  2008-12-05 15:12:42 -77.262447 -11.773147     -0.26  False   \n",
       "4  P1108_46_SV_T1  2008-12-05 15:12:43 -77.262482 -11.773217     -0.22  False   \n",
       "5  P1108_46_SV_T1  2008-12-05 15:12:44 -77.262517 -11.773293     -0.29  False   \n",
       "6  P1108_46_SV_T1  2008-12-05 15:12:45 -77.262518 -11.773372     -0.19  False   \n",
       "\n",
       "   step_speed  step_direction  dive   lon_std   lat_std  step_speed_std  \\\n",
       "2    7.224129        3.984122     0  1.452647  1.542331        0.246157   \n",
       "3    8.366185       13.285157     0  1.452565  1.541362        0.285681   \n",
       "4    8.675223       22.346828     0  1.451995  1.540458        0.296376   \n",
       "5    9.279737       -1.813227     0  1.451424  1.539476        0.317297   \n",
       "6    8.794348      -23.557701     0  1.451408  1.538456        0.300499   \n",
       "\n",
       "   step_direction_cos  step_direction_sin  \n",
       "2            0.997583            0.069480  \n",
       "3            0.973238            0.229798  \n",
       "4            0.924899            0.380212  \n",
       "5            0.999499           -0.031641  \n",
       "6            0.916658           -0.399672  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIP6-H4JLlCD"
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3JkDG3IbLlCD"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16\n",
    "learning_rate = 0.01\n",
    "variable = ('lon_std', 'lat_std', 'step_speed_std', 'step_direction_cos', 'step_direction_sin')\n",
    "window = 600\n",
    "rescale = 10\n",
    "\n",
    "train_set = TrajDataSet(data_train, window, variable, transform = transforms.Compose([Rescale(rescale), ToTensor()]))\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers = 0, shuffle = True, drop_last=True)\n",
    "\n",
    "validation_set = TrajDataSet(data_validation, window, variable, transform = transforms.Compose([Rescale(rescale), ToTensor()]))\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, num_workers = 0, shuffle = True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zK0f7AEsLlCD"
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.cnn_input_1 = nn.Sequential(\n",
    "            nn.Conv1d(5, 8, kernel_size = 11, stride = 1, padding = 5, dilation = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 8, kernel_size = 11, stride = 1, padding = 5, dilation = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.pooling_1 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size = 11, stride = 2, padding = 5, dilation = 1)\n",
    "        )\n",
    "\n",
    "        self.cnn_input_2 = nn.Sequential(\n",
    "            nn.Conv1d(8, 16, kernel_size = 11, stride = 1, padding = 5, dilation = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, kernel_size = 11, stride = 1, padding = 5, dilation = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.pooling_2 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size = 11, stride = 2, padding = 5, dilation = 1)\n",
    "        )\n",
    "\n",
    "        self.cnn_input_3 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, kernel_size = 11, stride = 1, padding = 5, dilation = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, kernel_size = 11, stride = 1, padding = 5, dilation = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.pooling_3 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size = 11, stride = 2, padding = 5, dilation = 4)\n",
    "        )\n",
    "\n",
    "        self.cnn_4 = nn.Sequential(\n",
    "            nn.Conv1d(32, 16, kernel_size = 11, stride = 1, padding = 5, dilation = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 8, kernel_size = 11, stride = 1, padding = 5, dilation = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 1, kernel_size = 11, stride = 1, padding = 5, dilation = 1)\n",
    "        )\n",
    "\n",
    "                \n",
    "    def forward(self, x):\n",
    "        out = x.squeeze(1)\n",
    "        out = self.cnn_input_1(out)\n",
    "        out = self.pooling_1(out)\n",
    "        out = self.cnn_input_2(out)\n",
    "        out = self.pooling_2(out)\n",
    "        out = self.cnn_input_3(out)\n",
    "        out = self.pooling_3(out)\n",
    "        out = self.cnn_4(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "def get_score(out, y):\n",
    "    out = 1*(out>0)\n",
    "    true_positive = np.mean(out[y == True].numpy()) \n",
    "    true_negative = 1-np.mean(out[y == False].numpy())\n",
    "    \n",
    "    return (round(true_positive*100) , round(true_negative*100))\n",
    "#     return (true_positive.detach().numpy().item() , true_negative.detach().numpy().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lt33oHwiLlCD",
    "outputId": "43ebd38a-a035-4609-9f60-4d4ece1da951"
   },
   "outputs": [],
   "source": [
    "# get sample\n",
    "x, y = next(iter(train_loader)) \n",
    "\n",
    "# Forward model\n",
    "model = ConvNet()\n",
    "out = model(x)\n",
    "\n",
    "# Loss and score\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight = torch.FloatTensor([15]))\n",
    "criterion(out, y)\n",
    "get_score(out, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TgPVDIiLlCD"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxyfYiWCLlCD"
   },
   "outputs": [],
   "source": [
    "# # switch to GPU\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x68uT4pHLlCE",
    "outputId": "7539da04-cf2b-49f6-d83e-0c7982ecad68",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "nb_epoch = 3\n",
    "weight = torch.FloatTensor([15])\n",
    "learning_rate = 0.01\n",
    "\n",
    "list_loss_train = []\n",
    "list_score_train = []\n",
    "\n",
    "list_loss_validation = []\n",
    "list_score_validation = []\n",
    "\n",
    "\n",
    "for epoch in range(nb_epoch):\n",
    "    learning_rate /= 10\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    i = 0\n",
    "    for batch, (x, y) in enumerate(train_loader):\n",
    "        i+=1\n",
    "    #     # send to GPU\n",
    "    #     x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Run the forward pass\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        # Backprop and perform optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        score = get_score(out,y)\n",
    "        list_loss_train.append(loss.item())\n",
    "        list_score_train.append(score)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Batch Loss: {}, Batch True Positive : {}, Batch True Negative : {} %'\n",
    "                    .format(epoch+1, nb_epoch, i + 1, len(train_loader), loss.item(), score[0], score[1]))\n",
    "            \n",
    "            \n",
    "    ### - Validation every epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        j = 0\n",
    "        for batch, (x, y) in enumerate(validation_loader):\n",
    "            j+= 1\n",
    "            # Run the forward pass\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            score = get_score(out,y)\n",
    "            list_loss_validation.append(loss.item())\n",
    "            list_score_validation.append(score)\n",
    "\n",
    "\n",
    "    global_loss = np.mean(list_loss_validation)\n",
    "    global_trueP = np.mean([tp for (tp, tn) in list_score_validation])\n",
    "    global_trueN = np.mean([tn for (tp, tn) in list_score_validation])\n",
    "\n",
    "    print('Validation -------------------------------------------------------------------------------------')\n",
    "    print('Epoch [{}/{}], Validation Loss: {}, Validation True Positive : {}, Validation True Negative : {} %'\n",
    "            .format(epoch+1, nb_epoch, global_loss, global_trueP, global_trueN))\n",
    "    print('------------------------------------------------------------------------------------------------')\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yecScZwrLlCE"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o1ms-K1XLlCE",
    "outputId": "0b410b5b-3383-428e-c190-f5376516b6a0"
   },
   "outputs": [],
   "source": [
    "dive_real = []\n",
    "dive_estim = []\n",
    "\n",
    "for i in data_test.trip.unique():\n",
    "    # create dataset for a trajectory\n",
    "    t = data_test[data_test.trip == i].copy()    \n",
    "    test_set = TrajDataSet(t, window, variable, transform = transforms.Compose([Rescale(rescale), ToTensor()]))\n",
    "    test_set_part = [test_set[i] for i in range(len(test_set)) if i%rescale == 0]\n",
    "    \n",
    "    # Test the model\n",
    "    estim = np.zeros(int((len(t))/rescale))\n",
    "    nb = np.zeros(int((len(t))/rescale))\n",
    "\n",
    "    list_out = []\n",
    "    model.eval()\n",
    "    k = 0\n",
    "    with torch.no_grad():\n",
    "        for (x, y) in test_set_part:\n",
    "            # Run the forward pass\n",
    "            out = model(x.unsqueeze(0))\n",
    "            \n",
    "            estim[k:k + round(window/rescale)] += out.squeeze().numpy()\n",
    "            nb[k:k + round(window/rescale)] += 1\n",
    "            k+=1\n",
    "\n",
    "    \n",
    "    # remove extra lines\n",
    "    t = t.drop(t.tail(len(t)%rescale ).index)\n",
    "    real = np.array([np.max(t.dive[i:i+rescale]) for i in range(len(t)) if i%rescale == 0])\n",
    "    \n",
    "    # add to list by trajectory\n",
    "    dive_real.append(real)\n",
    "    dive_estim.append(estim/nb)\n",
    "    \n",
    "    \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1sWjgLlLlCF"
   },
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "dive_plot = np.array([1*(estim[i]>threshold)  for i in range(len(estim)) for k in range(rescale) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "xZvcAyB8LlCF",
    "outputId": "f9952242-6555-4e31-f3dc-91d8dedd58cf"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.array(t.lon), np.array(t.lat))\n",
    "plt.scatter(t.lon[t.dive == 1], t.lat[t.dive == 1], c = 'orange')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.array(t.lon), np.array(t.lat))\n",
    "plt.scatter(t.lon[dive_plot == 1], t.lat[dive_plot == 1], c = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrbKoV8ULlCF"
   },
   "source": [
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NtCIrcPULlCF",
    "outputId": "f951a3d7-8b7e-4a09-e0a1-13bedcc0596b"
   },
   "outputs": [],
   "source": [
    "# globally\n",
    "all_real = 1* (np.hstack(dive_real)> 0)\n",
    "all_estim = 1* (np.hstack(dive_estim) > 0)\n",
    "\n",
    "true_positive = np.mean(all_estim[all_real == 1])\n",
    "true_negative = 1-np.mean(all_estim[all_real == 0])\n",
    "\n",
    "true_positive, true_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "TNDir8GcLlCG",
    "outputId": "c37930c6-0a82-4a5d-9e8e-67e9b32da883"
   },
   "outputs": [],
   "source": [
    "# for each trip\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "for i in range(len(dive_real)):\n",
    "    \n",
    "    real = 1*(dive_real[i]>0)\n",
    "    estim = 1*(dive_estim[i]>0)\n",
    "    \n",
    "    true_positive = np.mean(estim[real == 1])\n",
    "    true_negative = 1-np.mean(estim[real == 0])\n",
    "\n",
    "    ax.scatter(1-true_negative, true_positive, c = 'orange')\n",
    "    \n",
    "    print((true_positive, true_negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FLQbcx1LlCG"
   },
   "source": [
    "# ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5cYRSoWyLlCG"
   },
   "outputs": [],
   "source": [
    "# globally\n",
    "TP = []\n",
    "FP = []\n",
    "\n",
    "all_real = 1* (np.hstack(dive_real)> 0)    \n",
    "for tt in np.arange(-50, 50, 0.1):\n",
    "    all_estim = 1* (np.hstack(dive_estim) > tt)\n",
    "    true_positive = np.mean(all_estim[all_real == 1])\n",
    "    true_negative = 1-np.mean(all_estim[all_real == 0])\n",
    "    TP.append(true_positive)\n",
    "    FP.append(1-true_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "33SrgsZWLlCG",
    "outputId": "f862817f-ac22-48e1-b935-8a0c76721228",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.array(FP), np.array(TP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHvNO46jLlCG"
   },
   "outputs": [],
   "source": [
    "data = {'model': 'network_timeseries',\n",
    "        'FP':  FP,\n",
    "        'TP': TP\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame (data, columns = ['model','FP','TP'])\n",
    "\n",
    "df.to_csv('roc_network_timeseries.csv', index = False)\n",
    "# df.to_csv('./roc/roc_network_timeseries.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgtH95ei86MK"
   },
   "source": [
    "# Export Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rPN--xrw87iD",
    "outputId": "1fd23131-4b76-4861-81e0-a49873e151dc"
   },
   "outputs": [],
   "source": [
    "all_real = np.hstack(dive_real)\n",
    "all_estim = np.hstack(dive_estim)\n",
    "all_proba_estim = 1/(1+np.exp(-all_estim))\n",
    "all_proba_estim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8-qdqMrB6y2"
   },
   "outputs": [],
   "source": [
    "trip_name = data_test.trip.unique()\n",
    "trip_length = [len(i) for i in dive_estim]\n",
    "trip = [trip_name[i] for i in range(len(trip_name)) for k in range(trip_length[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIb8Imn9DC1e"
   },
   "outputs": [],
   "source": [
    "data = {'model': 'network_timeseries',\n",
    "        'trip':  trip,\n",
    "        'real': all_real,\n",
    "        'p': all_proba_estim\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame (data, columns = ['model','trip', 'real', 'p'])\n",
    "\n",
    "df.to_csv('prob_network_timeseries.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHkZnVQUEoV4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "dive_prediction_time_series_deep_network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
